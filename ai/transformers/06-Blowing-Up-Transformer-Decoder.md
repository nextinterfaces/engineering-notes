# ğŸ¬ Blowing Up the Transformer Decoder  
*"How the decoder turns context into translated words, one step at a time."*  

---

## ğŸ¯ Whatâ€™s the Decoderâ€™s Job?  

Take:  
- Context-rich vectors from the **Encoder** (English sentence meaning).  
- A **Start token** in the target language (e.g., Kannada).  

Output:  
- A translated sentence, generated word by word.  

```
Encoder Output (English) â”€â”€â–¶ Decoder â”€â”€â–¶ Kannada Translation
```

---

## ğŸ›  Step 1: Input Preparation  

Target sentence prep (during training):  
- Add **Start token**, **End token**, and **Padding**.  
- Shape: **Batch Ã— Max Length Ã— 512**  

```
Batch: many sentences at once (e.g., 32)  
Max Length: fixed size (e.g., 20 tokens)  
512: word vector dimension  
```

### Positional Encoding  
Since inputs are parallel, we add sine/cosine **position signals** to word embeddings.  

```
Word Embedding (512) + Position Vector (512) = Position-Aware Embedding
```

---

## ğŸ‘€ Step 2: Masked Multi-Head Self-Attention  

First block = decoder attends to **itself** (the partially generated sentence).  

1. **Q, K, V mapping**  
```
Each word â†’ Q, K, V (512-dim)
Q = What am I looking for?
K = What info do I have?
V = What info I give
```

2. **Split into 8 heads**  
```
512 â†’ 8 heads â†’ 64-dim each
```

3. **Masking trick (to avoid cheating)**  
```
Lower-triangular mask:
Word i â†’ can only see words â‰¤ i
Future words â†’ -âˆ (ignored)
```

4. **Attention math**  
```
Attention = softmax( (QÂ·Káµ€)/âˆšd ) Ã— V
```

Result = **8 heads of context-aware vectors** â†’ concatenated â†’ 512-dim.  

---

## ğŸ”— Step 3: Add & Norm #1  

```
Attention Output + Original Input â†’ Residual Connection
â†’ Layer Normalization (mean=0, stdâ‰ˆ1)
```

Keeps gradients flowing + stabilizes training.  

---

## ğŸŒ Step 4: Cross-Attention (Link to Encoder)  

Now the decoder looks at the **encoderâ€™s output (source sentence)**.  

- **Q** = from decoder (so far).  
- **K, V** = from encoder (full English sentence).  
- **No masking here!** Decoder can see the **whole source sentence**.  

```
Target word â†” All source words
```

Output â†’ Add & Norm again.  

---

## ğŸ”§ Step 5: Feed-Forward Network (FFN)  

```
512 â†’ Linear (expand to 1024) â†’ ReLU + Dropout â†’ Compress back to 512
```

- Learns nonlinear combos.  
- Add residual + LayerNorm.  

---

## ğŸ”„ Step 6: Repeat N Times  

- One decoder block = Masked Self-Attention + Cross-Attention + FFN.  
- Stack it **6â€“12 times** for real tasks.  

Diagram:  
```
[Decoder Input]
   â”‚
â”Œâ”€â”€â–¼â”€â”€â”
â”‚ Block â”‚ Ã— N
â””â”€â”€â–¼â”€â”€â”˜
   â”‚
[Context-rich decoder vectors]
```

---

## ğŸ Step 7: Final Output  

- 512-dim decoder vectors â†’ Linear layer â†’ Vocabulary size.  
- Softmax â†’ probability distribution for next word.  

---

## ğŸ§ª Training vs Inference  

### Training  
- All tokens processed in **parallel**.  
- Fast loss calculation vs ground-truth translation.  

### Inference (generation)  
- **One word at a time:**  
```
Start â†’ Predict "nanna" â†’ Feed back â†’ Predict "hesaru" â†’ ...
```

- Repeats until **End token**.  

---

## ğŸ“ Key Takeaways  

1. Decoder = **Start token in, words out.**  
2. **Masked Self-Attention:** Prevents peeking at future words.  
3. **Cross-Attention:** Decoder uses encoderâ€™s context.  
4. **FFN + Add & Norm:** Stabilize, refine, repeat.  
5. **Training = parallel**, **Inference = sequential.**  

---

ğŸ’¡ Memory Hook:  
**Decoder = storyteller:**  
- Remembers what it already said (masked self-attention).  
- Listens to the full source story (cross-attention).  
- Tells the translation, one word at a time.  
