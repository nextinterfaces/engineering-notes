# ğŸ’¥ Blowing Up the Transformer Encoder
*"Turning words into context-rich vectors, step by step."*  

---

## ğŸ¯ The Big Goal  

The **Encoder** takes a sentence â†’ outputs **context-rich vectors**.  
These vectors = â€œwhat each word means in context.â€  

Example:  
```
Input:  "my name is aj"
Output: 4 vectors (one per word), each 512-dim, context-aware
```

Why context matters:  
- â€œAjâ€ in *â€œmy name is ajâ€* vs. in *â€œaj is programmingâ€* â†’ same word, different meaning in context.  

---

## ğŸ›  Step 1: Input Preparation  

The input sentence has to be reshaped into something the network understands.  

### a) Tokenization + Padding  
```
Sentence â†’ ["my", "name", "is", "aj"]
```
- Pad if needed: `["my", "name", "is", "aj", "<PAD>", "<PAD>"]` (for fixed length).  

### b) One-Hot Encoding  
```
"my"   = [0,0,0,1,0,0,...,0]  (vocab size ~50k)
"name" = [0,0,1,0,0,0,...,0]
```
ğŸ‘‰ Very sparse and high-dimensional.  

### c) Embedding  
```
One-hot (50k) â†’ Dense vector (512)
```
Learned matrix compresses to 512-dim vector:  
```
"my"   â†’ [0.12, -0.43, ..., 0.56] (512-dim)
"name" â†’ [0.88,  0.01, ..., 0.33]
```

### d) Positional Encoding  
Since Transformer = parallel (not sequential), it has no order awareness.  
So we **add sin/cos signals** to each embedding.  

```
Word Embedding + Position Signal = Position-Aware Embedding
```

Diagram:  
```
my(emb) + pos(0) â†’ vector
name(emb) + pos(1) â†’ vector
is(emb) + pos(2) â†’ vector
aj(emb) + pos(3) â†’ vector
```

---

## ğŸ§  Step 2: Multi-Head Self-Attention  

Each token embedding (512-dim) is split into three roles:  

- **Query (Q):** â€œWhat am I looking for?â€  
- **Key (K):** â€œWhat info do I have?â€  
- **Value (V):** â€œWhat info do I pass on?â€  

```
Token â†’ Q, K, V  (each 512-dim)
```

### Attention Matrix  
For each head:  
```
Attention = softmax( (Q Â· Káµ€) / âˆšd ) Ã— V
```

- QÂ·Káµ€ = affinity between words.  
- Scale by âˆšd â†’ stabilize.  
- Softmax â†’ convert to probabilities.  
- Multiply by V â†’ weighted context.  

**Example:**  
```
Word = "name"
Q("name") Â· K("my")   â†’ 0.7  â†’ focus on "my"
Q("name") Â· K("is")   â†’ 0.2  â†’ less focus
Q("name") Â· K("aj")   â†’ 0.1  â†’ small focus
```
ğŸ‘‰ â€œnameâ€ attends mostly to â€œmy.â€  

### Multi-Head Trick  
- 512-dim split into 8 heads (each 64-dim).  
- Each head attends differently (syntax, long-distance, subject-object).  
- Outputs concatenated back â†’ 512-dim.  

Diagram:  
```
[Head1: 64] [Head2: 64] ... [Head8: 64] â†’ Concatenate â†’ 512
```

---

## ğŸ”— Step 3: Add & Norm #1  

```
Attention Output (512)
    +
Original Input (512)
    â†“
Residual Connection
    â†“
Layer Normalization
```

- Residual â†’ prevents vanishing gradients, lets info skip.  
- LayerNorm â†’ stabilize activations (mean=0, stdâ‰ˆ1).  

---

## ğŸ”§ Step 4: Feed-Forward + Add & Norm #2  

### Feed-Forward Network (FFN)  
```
Input (512)
 â†’ Expand (512 â†’ 1024)
 â†’ ReLU + Dropout
 â†’ Compress (1024 â†’ 512)
```
ğŸ‘‰ Learns more complex relationships beyond attention.  

### Add & Norm Again  
```
FFN output + previous â†’ LayerNorm
```

Keeps everything stable, helps deep stacking.  

---

## ğŸ”„ Step 5: Stacking Layers  

One encoder = all steps above.  
Transformers use **12â€“24 encoder blocks stacked**.  

Diagram:  
```
[Input Embeddings]
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Encoder â”‚ Ã— 12
â””â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”˜
     â”‚
[Context-rich vectors â†’ Decoder]
```

---

## ğŸ“ Key Takeaways  

1. **Embeddings + Position** = words with order.  
2. **Self-Attention** = each word looks at all others.  
3. **Multi-Head** = multiple perspectives.  
4. **Add & Norm** = skip connections + stable training.  
5. **FFN** = nonlinear mixing of features.  
6. **Stacked Encoders** = deeper, richer word understanding.  

---

ğŸ’¡ Memory Hook:  
**Encoder = language assembly line:**  
- Add meaning (embedding).  
- Add order (position).  
- Mix context (attention).  
- Refine (FFN).  
- Repeat (stack).  
