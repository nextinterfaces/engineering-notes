# ğŸ­ Multi-Head Attention in Transformers  
*"Why one pair of eyes isnâ€™t enough â€” models need many perspectives."*  

---

## ğŸš€ Transformer Recap (Sequence-to-Sequence)  

Transformers = **Encoder + Decoder** for sequence-to-sequence tasks.  

Example: Translate `"my name is aj"` â†’ Hindi.  

```
Input: "my name is aj"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Encoder   â”‚  (all words processed at once)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ context-rich vectors
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Decoder   â”‚  (generates step by step: "Mera" â†’ "Nam" â†’ "aj" â†’ "hey")
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  The Core Idea: Multi-Head Attention  

Each word vector â†’ split into 3 roles:  

- **Query (Q):** What Iâ€™m looking for  
- **Key (K):** What I can offer  
- **Value (V):** What I actually deliver  

```
Word â†’ [Q, K, V]
```

But one "attention head" may miss relationships.  
ğŸ‘‰ Solution: **multiple heads**, each with a different perspective.  

---

## ğŸ§© Splitting into Heads  

Example: Input vector = 512 dimensions.  

- Use **8 heads**.  
- Split 512 â†’ 8 Ã— 64.  
- Each head gets its own Q, K, V of size 64.  

```
Q (512) â†’ [Q1(64), Q2(64), â€¦ Q8(64)]
K (512) â†’ [K1(64), â€¦ K8(64)]
V (512) â†’ [V1(64), â€¦ V8(64)]
```

Each head runs attention separately â†’ produces its own 64-dim output per word.  

---

## ğŸ” Inside One Attention Head  

Steps for each head:  

1. **Scores:** `Q Â· Káµ€` â†’ affinity between words. (shape: Seq Ã— Seq)  
2. **Scale:** Divide by âˆš64 = 8 â†’ keep stable.  
3. **Mask (Decoder only):** Prevent peeking at future words â†’ fill upper-triangle with -âˆ.  
4. **Softmax:** Convert scores â†’ probabilities (rows sum = 1).  
5. **Weighted Sum:** Multiply probs Ã— V â†’ new context-rich word vectors.  

```
Attention = softmax( (QÂ·Káµ€) / âˆšd ) Ã— V
```

---

## ğŸ­ Multi-Head Magic  

Each head learns different relationships:  

```
Head 1: Subject â†” Verb
Head 2: Long-distance dependencies
Head 3: Word order nuances
...
Head 8: Subtle context
```

Then:  

```
[Head1(64), Head2(64), â€¦ Head8(64)] â†’ Concatenate â†’ 512-dim vector
```

Finally pass through a linear layer â†’ fuse information.  

---

## ğŸ“š PyTorch Implementation (Conceptual Flow)  

```
X: [Batch=1, Seq=4, Dim=512]

Step 1: Linear map â†’ QKV (1536 dims)
Step 2: Reshape â†’ (Heads=8, Seq=4, Q/K/V=64)
Step 3: Compute QÂ·Káµ€ for each head (4Ã—4 matrix)
Step 4: Scale by âˆš64
Step 5: Mask (if decoder)
Step 6: Softmax
Step 7: Multiply by V â†’ context vectors (Seq Ã— 64)
Step 8: Concatenate heads â†’ (Seq Ã— 512)
Step 9: Final linear (512Ã—512)
```

---

## ğŸ“ Key Takeaways  

1. Multi-head = multiple perspectives â†’ richer context.  
2. Each head = runs self-attention independently.  
3. Outputs are concatenated + transformed.  
4. Encoder & Decoder both rely on multi-head attention.  

---

ğŸ’¡ Memory Hook:  
**One head = one spotlight.  
Multi-head = concert lighting from every angle.**  
