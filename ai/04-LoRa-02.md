# ğŸ“„ LoRA Cheat Sheet (Low Rank Adaptation)  

*"Fine-tuning LLMs without the baggage."*  

---

## ğŸ§© Core Idea  
LoRA = **Low Rank Adaptation** â†’ shrink what you train by factorizing big weight matrices into **two tiny ones**.  

```
Big Weight (DÃ—K)  â‰ˆ  (DÃ—R) Ã— (RÃ—K)     where R â‰ª D, K
```

âœ… Train fewer parameters  
âœ… No slowdown at inference  

---

## ğŸ” Why LoRA?  

- âŒ Full Fine-Tuning: all weights updated â†’ huge cost + storage + forgetting  
- âŒ Adapters: small add-ons, but add latency  
- âœ… LoRA: parallel low-rank trick â†’ **tiny updates, zero latency**  

---

## ğŸ”Œ How LoRA Works  

```
Original Weights W   +   Low-Rank Update (AÃ—B)
```

- Insert A (DÃ—R) and B (RÃ—D) in **parallel** to frozen W  
- Apply to attention weights: **W_Q, W_K, W_V**  
- Train only A + B â†’ store just these small matrices  

---

## ğŸš€ Inference (Magic Part)  

At deployment:  

```
W_updated = W_original + (A Ã— B)
```

- Same shape as original W  
- No new layers, no overhead  
- Model = looks like fully fine-tuned, but trained with far fewer params  

---

## ğŸ† Benefits at a Glance  

- ğŸ“‰ Trainable params = tiny fraction of full model  
- ğŸ’¾ Storage = just the small A + B matrices per task  
- âš¡ Inference speed = same as original model (no latency)  
- ğŸ¯ Accuracy â‰ˆ full fine-tuning  

---

## ğŸ’¡ Memory Hooks  

- **Adapters = bolt-on gadgets â†’ slower.**  
- **LoRA = hidden upgrade chip â†’ faster.**  
- Think: *carrying a USB stick instead of a whole hard drive.*  

---
