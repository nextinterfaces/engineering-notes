# 📄 LoRA Cheat Sheet (Low Rank Adaptation)  

*"Fine-tuning LLMs without the baggage."*  

---

## 🧩 Core Idea  
LoRA = **Low Rank Adaptation** → shrink what you train by factorizing big weight matrices into **two tiny ones**.  

```
Big Weight (D×K)  ≈  (D×R) × (R×K)     where R ≪ D, K
```

✅ Train fewer parameters  
✅ No slowdown at inference  

---

## 🔍 Why LoRA?  

- ❌ Full Fine-Tuning: all weights updated → huge cost + storage + forgetting  
- ❌ Adapters: small add-ons, but add latency  
- ✅ LoRA: parallel low-rank trick → **tiny updates, zero latency**  

---

## 🔌 How LoRA Works  

```
Original Weights W   +   Low-Rank Update (A×B)
```

- Insert A (D×R) and B (R×D) in **parallel** to frozen W  
- Apply to attention weights: **W_Q, W_K, W_V**  
- Train only A + B → store just these small matrices  

---

## 🚀 Inference (Magic Part)  

At deployment:  

```
W_updated = W_original + (A × B)
```

- Same shape as original W  
- No new layers, no overhead  
- Model = looks like fully fine-tuned, but trained with far fewer params  

---

## 🏆 Benefits at a Glance  

- 📉 Trainable params = tiny fraction of full model  
- 💾 Storage = just the small A + B matrices per task  
- ⚡ Inference speed = same as original model (no latency)  
- 🎯 Accuracy ≈ full fine-tuning  

---

## 💡 Memory Hooks  

- **Adapters = bolt-on gadgets → slower.**  
- **LoRA = hidden upgrade chip → faster.**  
- Think: *carrying a USB stick instead of a whole hard drive.*  

---
