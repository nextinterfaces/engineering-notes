# ğŸŒŸ Self-Taught Reasoning (Star)  
*"How LLMs can learn to teach themselves reasoning."*  

---

## ğŸ˜µ The Problem: Why LLMs Struggle  

LLMs are trained on **predicting the next word** (language modeling).  
That makes them good at conversation, butâ€¦  

- âŒ They struggle with **arithmetic word problems**.  
- âŒ Direct fine-tuning on math or reasoning tasks often fails.  

We need something more than just answers.  
We needâ€¦ **reasoning.**  

---

## ğŸ§© Enter Chain-of-Thought (CoT)  

**CoT prompting** = Show the model not just the answer, but the **steps in between**.  

```
Q: 3 + 3 = ?  
Reasoning: Start with 3, add 3 more â†’ 6  
A: 6
```  

- âœ… Uses few-shot prompts.  
- âœ… Outperforms direct fine-tuning in some tasks.  
- âŒ Still limited.  

---

## ğŸŒŸ Enter Star: Self-Taught Reasoning  

Star = **Fine-tuning with rationales**.  
Instead of *only answers*, the model trains on **questions + reasoning + answers**.  

ğŸ‘‰ Like a student who doesnâ€™t just memorize answers but learns to write full solutions.  

---

## âš™ï¸ How Star Works  

Needs:  
- Pre-trained LLM.  
- Dataset of (Q â†’ A).  
- ~10 seed examples with **Q â†’ Reasoning â†’ A**.  

---

### ğŸ”„ The Two-Step Loop  

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  1. Generate Rationale   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
     If Answer âœ…    â”‚     If Answer âŒ
                     â–¼
        Add (Q, R, A) to dataset
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  2. Rationalize Step     â”‚
         â”‚  (reverse engineer R)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          Add corrected (Q, R, A)
```

This loop expands the dataset with rationales.  

---

### ğŸ“š Training Flow  

```
Seed (10 Q-R-A examples)
        â”‚
        â–¼
Generate + Rationalize â†’ Expanded Q-R-A dataset
        â”‚
        â–¼
Fine-tune LLM on Q-R-A
        â”‚
        â–¼
Model learns reasoning!
```

---

## ğŸ“Š Performance  

- **Arithmetic:** Rationalization accelerates performance on multi-digit addition.  
- **Commonsense QA:** Star â‰ˆ GPT-3, but with a model **30Ã— smaller**.  

---

## âš ï¸ Limitations  

1. **Model size matters:** Small models (GPT-2) canâ€™t generate good rationales.  
2. **Rationale quality:** Sometimes answer = right but reasoning = nonsense.  
   - Hard to evaluate what a â€œgoodâ€ rationale is.  

---

## ğŸ“ Key Takeaways  

1. **CoT:** Prompt with reasoning â†’ better results.  
2. **Star:** Fine-tune with reasoning â†’ even better.  
3. **Rationalize step:** Wrong answers become learning opportunities.  
4. **Efficiency:** Star = GPT-3-level with much smaller models.  
5. **Limits:** Needs big enough base models + reliable rationales.  

---

ğŸ’¡ Memory Hook:  
CoT = â€œshow your work.â€  
Star = â€œteach yourself to always show your work.â€  
