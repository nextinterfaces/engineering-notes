# âš–ï¸ Layer Normalization in Transformers  
*"Keeping activations calm so Transformers can learn fast."*  

---

## ğŸš€ Context: Where LayerNorm Fits  

Transformers = Encoder + Decoder.  
Inside each block, after Multi-Head Attention â†’ we see **Add & Norm**.  

```
Input â†’ Embedding â†’ Positional Encoding â†’ Multi-Head Attention
         â”‚                                   â”‚
         â””â”€â”€â”€â”€â”€â”€ Residual Connection â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                   [ Add & Norm ]
```

---

## ğŸ”— Add (Residual Connections)  

- Take the **input before attention (X')** and **add it** to the attention output.  
- Purpose: keep information flowing, prevent vanishing gradients.  

```
Output = Attention(X') + X'
```

ğŸ‘‰ Residuals = skip shortcuts so deep nets donâ€™t â€œforgetâ€ signals.  

---

## âš–ï¸ Norm (Layer Normalization)  

Neural nets produce activations with wild ranges â†’ unstable training.  
Normalization = bring them to a calm, stable range.  

### Formula  

```
y = Î³ * ( (x - Î¼) / Ïƒ ) + Î²
```

- Î¼ = mean of activations (per layer, per word).  
- Ïƒ = standard deviation.  
- Î³, Î² = learnable scale & shift parameters.  

ğŸ‘‰ After normalization â†’ mean â‰ˆ 0, std â‰ˆ 1.  

---

## ğŸ§® Example  

Word embedding dims = [0.2, 0.1, 0.3]  

- Î¼ = (0.2+0.1+0.3)/3 = 0.2  
- Ïƒ = std([0.2,0.1,0.3])  
- Normalize each â†’ (x - Î¼)/Ïƒ  
- Add Îµ in denominator to avoid divide-by-zero.  
- Scale + shift with Î³, Î².  

Result = stable, normalized vector.  

---

## ğŸ“Š Why It Matters  

- âš¡ Faster training (no exploding/vanishing activations).  
- ğŸ¯ More stable gradient updates.  
- ğŸ”„ Works across each word vector in the sequence.  

---

## ğŸ“ Key Takeaways  

1. **Add & Norm** = Residual connection + LayerNorm.  
2. Residual keeps info flow alive.  
3. LayerNorm stabilizes training by normalizing activations.  
4. Î³ & Î² = learnable tweaks for flexibility.  
5. Without LayerNorm â†’ Transformers would struggle to converge.  

---

ğŸ’¡ Memory Hook:  
**Residuals = shortcuts.  
LayerNorm = meditation for neurons (stay calm, balanced).**  
