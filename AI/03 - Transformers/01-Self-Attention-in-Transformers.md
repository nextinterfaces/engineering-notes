# ğŸ§  Self-Attention in Transformers  
*"How models stopped reading one word at a time and learned to look everywhere at once."*  

---

Insipired by https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4

## ğŸ˜µ The Old Days: RNNs and Their Struggles  

RNNs were once the go-to for sequences (like sentences).  
But they came with two big problems:  

1. **Slowness** ğŸ¢  
   - Words processed one by one.  
   - Training uses truncated backpropagation through time â†’ slow & painful.  

2. **Context Limits** ğŸ¯  
   - A wordâ€™s meaning depends on both left & right context.  
   - RNNs mostly looked **one way** (past â†’ future).  
   - Even bi-RNNs split context into two halves (L2R + R2L) â†’ not true holistic meaning.  

ğŸ‘‰ Result: Weak representations, long training.  

---

## ğŸ’¡ The Fix: Attention (and Self-Attention)  

Attention lets each word **look at every other word** in the sequence.  
When itâ€™s applied to the same input sentence â†’ **Self-Attention**.  

```
Word_i â†’ looks at {Word_1, Word_2, ..., Word_n}
```

Now every word representation = context-rich, not isolated.  

---

## ğŸš€ Transformers: Attention at the Core  

Transformers fixed RNN problems:  

- âš¡ Process inputs in parallel â†’ GPU-friendly.  
- ğŸ”„ Use **multi-headed self-attention** to capture full context.  

### Architecture (Sequence-to-Sequence)  

```
Input Sentence ("my name is aj")
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Encoder   â”‚  â† all words processed together
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚  context-rich vectors
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Decoder   â”‚  â† generates outputs step by step
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
Output: "à²¨à²¨à³à²¨ à²¹à³†à²¸à²°à³ à²…à²œà³" (Kannada)
```

---

## ğŸ” Self-Attention: The Intuition  

For each word, the model makes 3 vectors:  

- **Query (Q):** What am I looking for?  
- **Key (K):** What info do I have?  
- **Value (V):** What info I actually pass along.  

```
Word â†’ [Q, K, V]
```

### Step-by-Step  

1. **Affinity (QÂ·Káµ€):** Compare every word with every other word.  
   â†’ builds an attention matrix.  

2. **Scaling:** Divide by âˆšd (dimension size) â†’ stabilize values.  

3. **Masking (Decoder only):** Prevents â€œcheatingâ€ by looking ahead.  
   - Future words masked with -âˆ above diagonal.  

4. **Softmax:** Convert to probabilities (rows sum to 1).  

5. **Context Vectors:** Multiply probs Ã— Value matrix (V).  
   - Produces new vectors with richer context.  

---

## ğŸ­ Multi-Head Attention  

One head = one perspective.  
But meaning is multi-dimensional â†’ we use **multiple heads**.  

```
Head 1: focus on syntax  
Head 2: focus on subject-object relations  
Head 3: focus on long-range dependencies  
...  
Stack them â†’ Multi-Head Attention
```

---

## ğŸ“ Key Takeaways  

1. RNNs = slow, limited context.  
2. Attention = every word looks at every other word.  
3. Self-Attention = applied to the same sentence.  
4. Transformer = parallel + multi-head self-attention.  
5. Multi-Head = many â€œviewsâ€ of relationships combined.  

---

ğŸ’¡ Memory Hook:  
**RNNs = single-file reading line by line.  
Transformers = everyone in class looking at everyone elseâ€™s notes at once.**  
