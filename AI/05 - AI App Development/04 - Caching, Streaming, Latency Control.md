# âš¡ Performance Considerations in AI Systems  
**Caching, Streaming, Latency Control**  


## 1. Why Performance Matters?  

Imagine your AI app is like a **barista making coffee â˜•**:  
- If customers wait 5 minutes â†’ they leave.  
- If service is instant â†’ they come back daily.  

ğŸ‘‰ In AI-powered apps: **speed = user experience = revenue**.  

---  

## 2. Caching ğŸ—„ï¸  

Cache = **save results so you donâ€™t recompute**.  

### Types of Caching  
- **Exact-match cache**: If same input appears â†’ reuse output.  
- **Semantic cache**: Store embeddings â†’ reuse for semantically similar queries.  
- **Feature cache**: Store expensive preprocessing (e.g., embeddings, tokenization).  

```python
# Example: Redis caching for LLM results
import redis, hashlib
r = redis.Redis()

def cached_inference(prompt, model):
    key = hashlib.sha256(prompt.encode()).hexdigest()
    if r.exists(key):
        return r.get(key).decode()
    else:
        result = model.generate(prompt)
        r.set(key, result, ex=3600)  # expire in 1 hour
        return result
```  

âœ… Reduces API cost, latency.  
âŒ Risk of stale responses.  

---  

## 3. Streaming ğŸ“¡  

Instead of waiting for full response â†’ stream partial output.  

### Benefits  
- Improves **perceived latency** (users see progress).  
- Crucial for chatbots, real-time apps.  

```python
# FastAPI streaming response example
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

def generate():
    text = "Hello world from AI"
    for word in text.split():
        yield word + " "
        
@app.get("/stream")
def stream():
    return StreamingResponse(generate(), media_type="text/plain")
```  

âœ… Keeps users engaged.  
âŒ More complex infra (websockets, chunked responses).  

---  

## 4. Latency Control ğŸ›ï¸  

Latency budget = **upper bound of response time**.  

### Techniques  
1. **Batching** â†’ process multiple requests in one GPU call.  
2. **Distillation & quantization** â†’ smaller models = faster inference.  
3. **Timeouts** â†’ fail fast if API/tool too slow.  
4. **Load balancing** â†’ distribute requests across replicas.  
5. **Asynchronous APIs** â†’ free up resources while waiting.  

```python
# Example: timeout wrapper
import signal

class TimeoutException(Exception): pass

def handler(signum, frame): raise TimeoutException()

signal.signal(signal.SIGALRM, handler)
signal.alarm(2)  # max 2s
try:
    result = model.predict(x)
except TimeoutException:
    result = "Fallback response"
```  

âœ… Guarantees SLA.  
âŒ Trade-off: may return incomplete/approx answers.  

---  

## 5. System Design Text Diagram ğŸ—ï¸  

```
[User Request]  
     â†“  
 [Cache?] â†’ Hit â†’ Return Fast  
     â†“ Miss  
 [Preprocessing Cache]  
     â†“  
 [Model Inference]  
     â†“  
 [Stream results to user]  
     â†“  
 [Monitor Latency + Apply Controls]
```  

---  

## 6. Challenges âš ï¸  

- Cache invalidation (when to refresh results?).  
- Balancing streaming vs batch throughput.  
- GPU utilization vs latency trade-offs.  
- User perception vs backend cost.  

---  

## 7. Game Time ğŸ²  

Q1: Your chatbot repeats same answers often. Which optimization first?  
ğŸ‘‰ Add **caching**.  

Q2: Users complain about waiting for 10s before reply starts. Fix?  
ğŸ‘‰ Add **streaming**.  

Q3: Your SLA requires 500ms max latency, but model takes 2s. Fix?  
ğŸ‘‰ **Quantize model, use batching, set timeouts**.  

---  

## 8. Recap ğŸ‰  

- **Caching** = reuse results, cut cost & latency.  
- **Streaming** = improve perceived latency & UX.  
- **Latency control** = batching, distillation, load balancing, timeouts.  
