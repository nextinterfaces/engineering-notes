# ğŸ§ª Experiment Tracking: MLflow & Weights & Biases  

## 1. Why Experiment Tracking?  

Imagine youâ€™re a scientist in a kitchen lab ğŸ³.  
You try 100 recipes with different spices.  
- No notes? Youâ€™ll never remember the winning recipe.  
- Good notes? You can reproduce the dish perfectly.  

ğŸ‘‰ In ML, tracking = **hyperparameters, metrics, artifacts, code versions**.  

```
[Idea] â†’ [Experiment Run] â†’ [Tracking System] â†’ [Compare & Reproduce]
```  

---  

## 2. What to Track?  

- **Code version** (Git commit).  
- **Hyperparameters** (learning rate, batch size).  
- **Metrics** (accuracy, loss, F1, AUC).  
- **Artifacts** (models, plots, logs).  
- **Environment** (Python version, packages).  

---  

## 3. MLflow â€“ The Open Source All-rounder ğŸŒ  

- Run local or server mode.  
- Components: Tracking, Projects, Models, Registry.  

### Example: Log parameters & metrics  

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

X, y = ..., ...
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

with mlflow.start_run():
    model = RandomForestClassifier(n_estimators=100, max_depth=5)
    model.fit(X_train, y_train)
    acc = accuracy_score(y_val, model.predict(X_val))

    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 5)
    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(model, "model")
```  

- âœ… Easy to set up.  
- âœ… Works with multiple frameworks.  
- âŒ UI less modern than W&B.  

---  

## 4. Weights & Biases (W&B) â€“ The Collaboration Hub ğŸ¤  

- Cloud-first platform.  
- Powerful UI for charts, comparisons, collaboration.  
- Great for teams working remotely.  

### Example: Log experiment with W&B  

```python
import wandb
import random

wandb.init(project="my_experiment", config={"lr": 0.001, "batch_size": 32})

for epoch in range(5):
    acc = random.random()
    loss = random.random()
    wandb.log({"epoch": epoch, "accuracy": acc, "loss": loss})

wandb.finish()
```  

- âœ… Fantastic dashboards.  
- âœ… Easy sweeps (hyperparameter search).  
- âŒ Requires cloud (though local mode exists).  

---  

## 5. Text Diagram: Comparing MLflow & W&B  

```
MLflow â†’ Open-source, self-host, flexible, registry focus  
W&B    â†’ Cloud-native, collaboration, visual dashboards  
```  

---  

## 6. Integration with CI/CD & MLOps  

- **MLflow** integrates well with Kubeflow, Airflow, Jenkins.  
- **W&B** integrates well with HuggingFace, PyTorch Lightning, Keras.  
- Both support APIs for automation & pipelines.  

---  

## 7. Game Time ğŸ²  

Q: You need to compare 100 hyperparameter runs with your teammate in another country. Which tool is better?  

ğŸ‘‰ Answer: **Weights & Biases** (collaboration + cloud).  

---  

## 8. Recap ğŸ‰  

- Tracking ensures **reproducibility** and **visibility**.  
- **MLflow**: open-source, self-host, registry for model lifecycle.  
- **W&B**: cloud-native, great dashboards, team collaboration.  

âš¡ Together, they make experiment management robust & reliable.  
