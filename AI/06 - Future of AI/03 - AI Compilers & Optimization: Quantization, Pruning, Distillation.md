# âš™ï¸ AI Compilers & Optimization: Quantization, Pruning, Distillation  

## 1. Why AI Compilers & Optimization?  

Imagine trying to fit a **big couch ğŸ›‹ï¸ into a tiny apartment**.  
- The couch = massive AI model.  
- The apartment = mobile device / edge GPU.  

ğŸ‘‰ Without optimization, models are too slow, big, or expensive.  
ğŸ‘‰ With compilers + optimizations, we **shrink + speed up** models while keeping accuracy.  

---  

## 2. AI Compilers ğŸ”¨  

AI compilers = special toolchains that **translate models into efficient code** for CPUs, GPUs, TPUs.  

Examples:  
- **ONNX Runtime**  
- **TVM**  
- **TensorRT (NVIDIA)**  
- **XLA (TensorFlow/JAX)**  

Pipeline:  

```
[Trained Model] â†’ [Compiler IR] â†’ [Optimizations: fuse ops, parallelize] â†’ [Target Hardware Binary]
```  

---  

## 3. Quantization ğŸ”¢  

Quantization = reduce precision of numbers (weights/activations).  

- FP32 â†’ FP16 â†’ INT8 â†’ sometimes binary.  
- Goal: **smaller model, faster inference, less memory**.  

```python
# PyTorch static quantization example
import torch
from torch.quantization import quantize_dynamic

model_fp32 = torch.load("model.pth")
model_int8 = quantize_dynamic(model_fp32, {torch.nn.Linear}, dtype=torch.qint8)
torch.save(model_int8, "model_quantized.pth")
```  

âœ… Speedup on CPUs/edge devices.  
âŒ Accuracy drop if aggressive.  

---  

## 4. Pruning âœ‚ï¸  

Pruning = remove unnecessary weights or neurons.  

- **Unstructured pruning**: zero out small weights.  
- **Structured pruning**: remove whole neurons/filters.  

```python
# PyTorch pruning example
import torch.nn.utils.prune as prune

prune.random_unstructured(model.fc, name="weight", amount=0.3)
```  

âœ… Shrinks model size, speeds inference.  
âŒ Risk of accuracy loss.  

---  

## 5. Distillation ğŸ”¥  

Knowledge distillation = train a **smaller student model** to mimic a large **teacher model**.  

```
[Teacher Model Output (soft labels)]  
           â†“  
[Student Model learns to imitate teacher]  
```  

```python
# Distillation sketch
teacher.eval()
for x, y in dataloader:
    with torch.no_grad():
        soft_labels = teacher(x)
    student_loss = distill_loss(student(x), soft_labels)
    ...
```  

âœ… Keeps performance while reducing size.  
âŒ Needs training + teacher access.  

---  

## 6. Combined Workflow ğŸ—ï¸  

Typical optimization pipeline:  

```
Train big model â†’ Distill student â†’ Prune weights â†’ Quantize â†’ Compile â†’ Deploy
```  

---  

## 7. Text Diagram  

```
Optimization Axes:  
   - Quantization â†’ smaller numbers  
   - Pruning      â†’ fewer parameters  
   - Distillation â†’ smaller models mimic bigger ones  
```  

---  

## 8. Tools & Frameworks ğŸ› ï¸  

- **Quantization**: PyTorch, TensorFlow Lite, ONNX Runtime.  
- **Pruning**: PyTorch, SparseML.  
- **Distillation**: HuggingFace Transformers, DistilBERT, TinyBERT.  
- **Compilers**: TVM, TensorRT, XLA.  

---  

## 9. Game Time ğŸ²  

Q1: You want to deploy a BERT model on mobile. Which techniques first?  
ğŸ‘‰ Quantization + Distillation.  

Q2: You need to reduce GPU memory for inference by 50%. Which technique?  
ğŸ‘‰ Quantization or pruning.  

Q3: You want a small chatbot model trained from GPT-3. Which method?  
ğŸ‘‰ Distillation.  

---  

## 10. Recap ğŸ‰  

- **Compilers** â†’ map models to efficient hardware code.  
- **Quantization** â†’ lower precision, smaller + faster.  
- **Pruning** â†’ cut unnecessary weights.  
- **Distillation** â†’ small model learns from big one.  
- Combined â†’ deploy AI anywhere: cloud, edge, mobile.  

âš¡ Optimization = making AI **practical + efficient**.  
