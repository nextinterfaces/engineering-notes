# âš™ï¸ AI Compilers & Optimization: Quantization, Pruning, Distillation   

## 1. Why AI Compilers & Optimization?  

Imagine trying to fit a **big couch ğŸ›‹ï¸ into a tiny apartment**.  
- The couch = massive AI model.  
- The apartment = mobile device / edge GPU.  

ğŸ‘‰ Without optimization, models are too slow, big, or expensive.  
ğŸ‘‰ With compilers + optimizations, we **shrink + speed up** models while keeping accuracy.  

---  

## 2. AI Compilers ğŸ”¨  

AI compilers = special toolchains that **translate models into efficient code** for CPUs, GPUs, TPUs.  

Examples:  
- **ONNX Runtime**  
- **TVM**  
- **TensorRT (NVIDIA)**  
- **XLA (TensorFlow/JAX)**  

Pipeline:  

```
[Trained Model] â†’ [Compiler IR] â†’ [Optimizations: fuse ops, parallelize] â†’ [Target Hardware Binary]
```  

---  

## 3. Quantization ğŸ”¢  

Quantization = reduce precision of numbers (weights/activations).  

- FP32 â†’ FP16 â†’ INT8 â†’ sometimes binary.  
- Goal: **smaller model, faster inference, less memory**.  

```python
# PyTorch static quantization example
import torch
from torch.quantization import quantize_dynamic

model_fp32 = torch.load("model.pth")
model_int8 = quantize_dynamic(model_fp32, {torch.nn.Linear}, dtype=torch.qint8)
torch.save(model_int8, "model_quantized.pth")
```  

âœ… Speedup on CPUs/edge devices.  
âŒ Accuracy drop if aggressive.  

---  

## 4. Pruning âœ‚ï¸  

Pruning = remove unnecessary weights or neurons.  

- **Unstructured pruning**: zero out small weights.  
- **Structured pruning**: remove whole neurons/filters.  

```python
# PyTorch pruning example
import torch.nn.utils.prune as prune

prune.random_unstructured(model.fc, name="weight", amount=0.3)
```  

âœ… Shrinks model size, speeds inference.  
âŒ Risk of accuracy loss.  

---  

## 5. Distillation ğŸ”¥  

Knowledge distillation = train a **smaller student model** to mimic a large **teacher model**.  

```
[Teacher Model Output (soft labels)]  
           â†“  
[Student Model learns to imitate teacher]  
```  

```python
# Distillation sketch
teacher.eval()
for x, y in dataloader:
    with torch.no_grad():
        soft_labels = teacher(x)
    student_loss = distill_loss(student(x), soft_labels)
    ...
```  

âœ… Keeps performance while reducing size.  
âŒ Needs training + teacher access.  

---  

## 6. Combined Workflow ğŸ—ï¸  

Typical optimization pipeline:  

```
Train big model â†’ Distill student â†’ Prune weights â†’ Quantize â†’ Compile â†’ Deploy
```  

---  

## 7. Side-by-Side Workflow Diagram ğŸ–¼ï¸  

```
Quantization            Pruning                  Distillation
-------------           -------------            ---------------
FP32 Weights            Full Dense Model         Large Teacher Model
     â†“                        â†“                          â†“
Reduce Precision        Remove small weights      Train small Student Model
(FP16 / INT8)           or neurons (structured)   on Teacher's outputs
     â†“                        â†“                          â†“
Smaller + Faster        Smaller + Faster          Smaller + Faster
But some accuracy drop  Accuracy may drop          Accuracy mostly preserved
```  

---  

## 8. Tools & Frameworks ğŸ› ï¸  

- **Quantization**: PyTorch, TensorFlow Lite, ONNX Runtime.  
- **Pruning**: PyTorch, SparseML.  
- **Distillation**: HuggingFace Transformers, DistilBERT, TinyBERT.  
- **Compilers**: TVM, TensorRT, XLA.  

---  

## 9. Comparison Table ğŸ†š  

| Technique       | Size Reduction ğŸ“¦ | Speed Gain âš¡ | Accuracy Impact ğŸ¯ | Notes |
|-----------------|------------------|--------------|--------------------|-------|
| **Quantization** | 2â€“4x smaller     | 2â€“4x faster  | Small to medium drop | Great for edge/CPU inference |
| **Pruning**     | 2â€“10x smaller    | 2â€“5x faster  | Medium drop if aggressive | Structured pruning helps hardware efficiency |
| **Distillation** | 2â€“5x smaller     | 2â€“3x faster  | Minimal drop (sometimes better) | Requires retraining with teacher model |  

---  

## 10. Case Study: DistilBERT ğŸ§ª  

DistilBERT is a **distilled version of BERT**:  

- Teacher = BERT (110M parameters).  
- Student = DistilBERT (66M parameters).  
- Training method = **knowledge distillation**.  

### Key Results  
- **40% smaller**.  
- **60% faster inference**.  
- **Retains ~97% of BERTâ€™s performance** on NLP benchmarks.  

### How it Worked  
1. Train student on BERTâ€™s soft labels (logits).  
2. Add loss from ground-truth labels.  
3. Use temperature scaling for smoother probabilities.  
4. Optimize trade-off: accuracy vs efficiency.  

```
DistilBERT = Student Model  
   â†“  
Trained with loss = Î± * CE(hard labels) + Î² * KL(soft labels from BERT)
```  

ğŸ‘‰ Proof that distillation can shrink models massively **without big accuracy loss**.  

---  

## 11. Game Time ğŸ²  

Q1: You want to deploy a BERT model on mobile. Which techniques first?  
ğŸ‘‰ Quantization + Distillation.  

Q2: You need to reduce GPU memory for inference by 50%. Which technique?  
ğŸ‘‰ Quantization or pruning.  

Q3: You want a small chatbot model trained from GPT-3. Which method?  
ğŸ‘‰ Distillation.  

Q4: What technique created DistilBERT?  
ğŸ‘‰ **Knowledge Distillation**.  

---  

## 12. Recap ğŸ‰  

- **Compilers** â†’ map models to efficient hardware code.  
- **Quantization** â†’ lower precision, smaller + faster.  
- **Pruning** â†’ cut unnecessary weights.  
- **Distillation** â†’ small model learns from big one.  
- **Workflow diagram** shows side-by-side differences.  
- **Case study (DistilBERT)** = real-world proof of distillation success.  

âš¡ Optimization = making AI **practical + efficient**.  
